# -*- coding: utf-8 -*-
"""Analyze employee feedback surveys to identify areas of improvement

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tkp0oJaWzVwxYHdLmUZDaWxE3zx-Dbew
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
uploads = files.upload()

uploads = files.upload()

print(df.columns)

uploads = files.upload()

uploads = files.upload()

print(df.columns)

uploads = files.upload()

print(df.columns)

pip install pandas matplotlib seaborn nltk wordcloud textblob scikit-learn

from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Display the first few rows
print(df.head())

# Display summary information
print(df.info())

# Display the column names
print(df.columns)

print(df.isnull().sum())

from textblob import TextBlob

# Sentiment polarity score for each feedback
df['sentiment'] = df['feedback'].apply(lambda x: TextBlob(x).sentiment.polarity)

def categorize_sentiment(polarity):
    if polarity > 0:
        return 'positive'
    elif polarity < 0:
        return 'negative'
    else:
        return 'neutral'

df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)

plt.figure(figsize=(8, 6))
sns.countplot(x='sentiment_category', data=df)
plt.title('Distribution of Sentiment Categories')
plt.show()

nltk.download('punkt')

import nltk

# Download the stopwords
nltk.download('stopwords')
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
from nltk.corpus import stopwords

nltk.download('stopwords')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load dataset


# Data Preprocessing
df.dropna(subset=['feedback'], inplace=True)
df['feedback'] = df['feedback'].str.lower()
df['feedback_len'] = df['feedback'].apply(len)
df['num_of_sent'] = df['feedback'].apply(lambda x: len(x.split('.')))

# Sentiment Analysis
df['sentiment'] = df['feedback'].apply(lambda x: TextBlob(x).sentiment.polarity)

def categorize_sentiment(polarity):
    if polarity > 0:
        return 'positive'
    elif polarity < 0:
        return 'negative'
    else:
        return 'neutral'

df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)

# Word Cloud
stop_words = set(stopwords.words('english'))
df['feedback_clean'] = df['feedback'].apply(lambda x: " ".join([word for word in word_tokenize(x) if word.isalnum() and word not in stop_words]))

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(df['feedback_clean']))
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Employee Feedback')
plt.show()

# Most frequent words
all_words = " ".join(df['feedback_clean']).split()
word_counts = Counter(all_words)
print(word_counts.most_common(20))

from collections import Counter

# Tokenize the cleaned feedback text
all_words = " ".join(df['feedback_clean']).split()
word_counts = Counter(all_words)

# Display the 20 most common words
print(word_counts.most_common(20))

plt.figure(figsize=(8, 6))
sns.boxplot(x='nine_box_category', y='sentiment', data=df)
plt.title('Sentiment by Nine Box Category')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='label', y='sentiment', data=df)
plt.title('Sentiment by Label')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
dtm = vectorizer.fit_transform(df['feedback_clean'])

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(dtm)

for index, topic in enumerate(lda.components_):
    print(f'Top 10 words for Topic #{index+1}:')
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])
    print("\n")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Load dataset


# Data Preprocessing
df.dropna(subset=['feedback'], inplace=True)
df['feedback'] = df['feedback'].str.lower()
df['feedback_len'] = df['feedback'].apply(len)
df['num_of_sent'] = df['feedback'].apply(lambda x: len(x.split('.')))

# Sentiment Analysis
df['sentiment'] = df['feedback'].apply(lambda x: TextBlob(x).sentiment.polarity)

def categorize_sentiment(polarity):
    if polarity > 0:
        return 'positive'
    elif polarity < 0:
        return 'negative'
    else:
        return 'neutral'

df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)

# Word Cloud
stop_words = set(stopwords.words('english'))
df['feedback_clean'] = df['feedback'].apply(lambda x: " ".join([word for word in word_tokenize(x) if word.isalnum() and word not in stop_words]))

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(df['feedback_clean']))
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Employee Feedback')
plt.show()

# Most frequent words
all_words = " ".join(df['feedback_clean']).split()
word_counts = Counter(all_words)
print(word_counts.most_common(20))

# Correlation and Insights
plt.figure(figsize=(8, 6))
sns.boxplot(x='nine_box_category', y='sentiment', data=df)
plt.title('Sentiment by Nine Box Category')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='label', y='sentiment', data=df)
plt.title('Sentiment by Label')
plt.show()

# Topic Modeling
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
dtm = vectorizer.fit_transform(df['feedback_clean'])

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(dtm)

for index, topic in enumerate(lda.components_):
    print(f'Top 10 words for Topic #{index+1}:')
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])
    print("\n")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')



# Display the first few rows of the dataframe
print(df.head())

# Data Preprocessing
df.dropna(subset=['feedback'], inplace=True)
df['feedback'] = df['feedback'].str.lower()
df['feedback_len'] = df['feedback'].apply(len)
df['num_of_sent'] = df['feedback'].apply(lambda x: len(x.split('.')))

# Sentiment Analysis
df['sentiment'] = df['feedback'].apply(lambda x: TextBlob(x).sentiment.polarity)

def categorize_sentiment(polarity):
    if polarity > 0:
        return 'positive'
    elif polarity < 0:
        return 'negative'
    else:
        return 'neutral'

df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)

# Clean up the feedback column (remove stopwords)
stop_words = set(nltk.corpus.stopwords.words('english'))
df['feedback_clean'] = df['feedback'].apply(lambda x: " ".join([word for word in nltk.word_tokenize(x) if word.isalnum() and word not in stop_words]))

# Finalized dataset ready for analysis
print(df.head())

plt.figure(figsize=(8, 6))
sns.countplot(x='sentiment_category', data=df, palette='coolwarm')
plt.title('Distribution of Sentiment Categories')
plt.xlabel('Sentiment Category')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df['feedback_len'], bins=30, kde=True)
plt.title('Distribution of Feedback Length')
plt.xlabel('Length of Feedback (characters)')
plt.ylabel('Frequency')
plt.show()

from wordcloud import WordCloud

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(df['feedback_clean']))
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Employee Feedback')
plt.show()

# Save the processed data to a new Excel file
df.to_excel('processed_employee_feedback.xlsx', index=False)